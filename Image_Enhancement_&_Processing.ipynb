{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, array_to_img\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "metadata": {
        "id": "gm5YMgOxQ_Ow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path='/content/drive/MyDrive/Images Dataset/unsplash-images-collection'"
      ],
      "metadata": {
        "id": "JOlbVydhSH6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "oJfEfbaKST4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Preprocessing\n",
        "def load_and_preprocess_images(image_path, img_size=(128, 128)):\n",
        "    \"\"\"\n",
        "    Load and preprocess images from the dataset.\n",
        "    Args:\n",
        "        dataset_path (str): Path to the dataset directory.\n",
        "        img_size (tuple): Target image size (height, width).\n",
        "    Returns:\n",
        "        np.array: Preprocessed images as a numpy array.\n",
        "    \"\"\"\n",
        "    images = []\n",
        "    for img_name in os.listdir(image_path):\n",
        "        img_path = os.path.join(image_path, img_name)\n",
        "        try:\n",
        "            # Load image\n",
        "            img = Image.open(img_path).convert('L')  # Convert to grayscale\n",
        "            img = img.resize(img_size)  # Resize image\n",
        "            img = img_to_array(img)  # Convert to numpy array\n",
        "            img = img / 255.0  # Normalize to [0, 1]\n",
        "            images.append(img)\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping {img_name}: {e}\")\n",
        "    return np.array(images)"
      ],
      "metadata": {
        "id": "OlDXWUmjSYkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Traditional Image Enhancement Techniques\n",
        "def apply_traditional_enhancement(image):\n",
        "    \"\"\"\n",
        "    Apply traditional image enhancement techniques.\n",
        "    Args:\n",
        "        image (np.array): Input image.\n",
        "    Returns:\n",
        "        np.array: Enhanced image.\n",
        "    \"\"\"\n",
        "    # Histogram Equalization\n",
        "    image = cv2.equalizeHist((image * 255).astype(np.uint8))\n",
        "    # Gaussian Blurring\n",
        "    image = cv2.GaussianBlur(image, (5, 5), 0)\n",
        "    return image / 255.0"
      ],
      "metadata": {
        "id": "B2uiHjN5SxBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Deep Learning-Based Image Enhancement (Autoencoder)\n",
        "def build_autoencoder(input_shape=(128, 128, 1)):\n",
        "    \"\"\"\n",
        "    Build a convolutional autoencoder for image enhancement.\n",
        "    Args:\n",
        "        input_shape (tuple): Input image shape (height, width, channels).\n",
        "    Returns:\n",
        "        Model: Autoencoder model.\n",
        "    \"\"\"\n",
        "    input_img = Input(shape=input_shape) # Define the input tensor\n",
        "    # Encoder\n",
        "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img) # Pass the input tensor here\n",
        "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "    encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
        "     # Decoder\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(encoded)\n",
        "    x = UpSampling2D((2, 2))(x)\n",
        "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = UpSampling2D((2, 2))(x)\n",
        "    decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "    # Autoencoder model\n",
        "    autoencoder = Model(input_img, decoded)\n",
        "    autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "    return autoencoder"
      ],
      "metadata": {
        "id": "MtL1Bbl_S4p2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GAN-Based Super-Resolution (Simplified)\n",
        "def build_gan(input_shape=(128, 128, 1)):\n",
        "    \"\"\"\n",
        "    Build a simplified GAN for super-resolution.\n",
        "    Args:\n",
        "        input_shape (tuple): Input image shape (height, width, channels).\n",
        "    Returns:\n",
        "        tuple: Generator and Discriminator models.\n",
        "    \"\"\"\n",
        "    # Generator\n",
        "    input_img = Input(shape=input_shape)\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(input_img)\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "    generator = Model(input_img, x)\n",
        "\n",
        "    # Discriminator\n",
        "    input_img_disc = Input(shape=input_shape)\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(input_img_disc)\n",
        "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "    x = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "    discriminator = Model(input_img_disc, x)\n",
        "\n",
        "    # Compile Discriminator\n",
        "    discriminator.compile(optimizer=Adam(learning_rate=0.0002), loss='binary_crossentropy')\n",
        "\n",
        "    return generator, discriminator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "JK4rjTCLTIhF",
        "outputId": "8d87afba-d832-427e-8410-f06c94e456be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-33-cb743f7e28d8>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-33-cb743f7e28d8>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    GAN-Based Super-Resolution (Simplified)\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#GAN-Based Super-Resolution (Simplified)\n",
        "def build_gan(input_shape=(128, 128, 1)):\n",
        "    \"\"\"\n",
        "    Build a simplified GAN for super-resolution.\n",
        "    Args:\n",
        "        input_shape (tuple): Input image shape (height, width, channels).\n",
        "    Returns:\n",
        "        tuple: Generator and Discriminator models.\n",
        "    \"\"\"\n",
        "    # Generator\n",
        "    input_img = Input(shape=input_shape)\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(input_img)\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "    generator = Model(input_img, x)\n",
        "\n",
        "    # Discriminator\n",
        "    input_img_disc = Input(shape=input_shape)\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(input_img_disc)\n",
        "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "    x = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "    discriminator = Model(input_img_disc, x)\n",
        "\n",
        "    # Compile Discriminator\n",
        "    discriminator.compile(optimizer=Adam(learning_rate=0.0002), loss='binary_crossentropy')\n",
        "\n",
        "    return generator, discriminator\n",
        "\n"
      ],
      "metadata": {
        "id": "G9I-AHeeT2E3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluation Metrics\n",
        "def evaluate_image_quality(original, enhanced):\n",
        "    \"\"\"\n",
        "    Evaluate image quality using PSNR and SSIM.\n",
        "    Args:\n",
        "        original (np.array): Original image.\n",
        "        enhanced (np.array): Enhanced image.\n",
        "    Returns:\n",
        "        tuple: PSNR and SSIM scores.\n",
        "    \"\"\"\n",
        "    psnr_score = psnr(original, enhanced, data_range=1)\n",
        "    ssim_score = ssim(original, enhanced, data_range=1, win_size=3)\n",
        "    return psnr_score, ssim_score#"
      ],
      "metadata": {
        "id": "J9sSSn2JUAT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Load dataset\n",
        "    # image_path = \"unsplash-images-collection\"  # This was the incorrect relative path\n",
        "    image_path = '/content/drive/MyDrive/Images Dataset/unsplash-images-collection' # Use the full path instead\n",
        "    images = load_and_preprocess_images(image_path=image_path) # Pass the full path to the function\n",
        "\n",
        "    # Build and apply the autoencoder\n",
        "    autoencoder = build_autoencoder()  # Create an instance of the autoencoder\n",
        "    enhanced_autoencoder = autoencoder.predict(images) # Enhance the images\n",
        "\n",
        "# Apply traditional enhancement\n",
        "enhanced_traditional = np.array([apply_traditional_enhancement(img) for img in images])\n",
        "\n",
        "# Train Autoencoder\n",
        "autoencoder = build_autoencoder()\n",
        "autoencoder.fit(images, images, epochs=50, batch_size=32, validation_split=0.2, callbacks=[EarlyStopping(patience=3)])\n",
        "enhanced_autoencoder = autoencoder.predict(images)\n",
        "\n",
        "# Train GAN (Simplified)\n",
        "generator, discriminator = build_gan()\n",
        "# Note: GAN training is simplified for demonstration purposes.\n",
        "\n",
        "# Evaluate and visualize results\n",
        "for i in range(min(5, len(images))):  # Display first 5 images\n",
        "    original = images[i]\n",
        "    traditional = enhanced_traditional[i]\n",
        "    autoencoded = enhanced_autoencoder[i]\n",
        "\n",
        "    # Evaluate quality\n",
        "    psnr_traditional, ssim_traditional = evaluate_image_quality(original, traditional)\n",
        "    psnr_autoencoded, ssim_autoencoded = evaluate_image_quality(original, autoencoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iGeDqoRSUHZQ",
        "outputId": "f9fb05b6-b5b4-4a25-87fd-1b5e0be43af9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 559ms/step\n",
            "Epoch 1/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 2s/step - loss: 0.0722 - val_loss: 0.0208\n",
            "Epoch 2/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 2s/step - loss: 0.0168 - val_loss: 0.0107\n",
            "Epoch 3/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - loss: 0.0097 - val_loss: 0.0072\n",
            "Epoch 4/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 2s/step - loss: 0.0075 - val_loss: 0.0063\n",
            "Epoch 5/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 2s/step - loss: 0.0065 - val_loss: 0.0056\n",
            "Epoch 6/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 2s/step - loss: 0.0059 - val_loss: 0.0056\n",
            "Epoch 7/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - loss: 0.0055 - val_loss: 0.0048\n",
            "Epoch 8/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - loss: 0.0049 - val_loss: 0.0042\n",
            "Epoch 9/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 2s/step - loss: 0.0044 - val_loss: 0.0040\n",
            "Epoch 10/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 2s/step - loss: 0.0042 - val_loss: 0.0038\n",
            "Epoch 11/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - loss: 0.0041 - val_loss: 0.0040\n",
            "Epoch 12/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - loss: 0.0039 - val_loss: 0.0034\n",
            "Epoch 13/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - loss: 0.0037 - val_loss: 0.0038\n",
            "Epoch 14/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - loss: 0.0037 - val_loss: 0.0034\n",
            "Epoch 15/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - loss: 0.0037 - val_loss: 0.0035\n",
            "Epoch 16/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 2s/step - loss: 0.0035 - val_loss: 0.0032\n",
            "Epoch 17/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 2s/step - loss: 0.0034 - val_loss: 0.0031\n",
            "Epoch 18/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - loss: 0.0034 - val_loss: 0.0032\n",
            "Epoch 19/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - loss: 0.0034 - val_loss: 0.0033\n",
            "Epoch 20/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 2s/step - loss: 0.0033 - val_loss: 0.0031\n",
            "Epoch 21/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 2s/step - loss: 0.0032 - val_loss: 0.0029\n",
            "Epoch 22/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 2s/step - loss: 0.0032 - val_loss: 0.0030\n",
            "Epoch 23/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - loss: 0.0032 - val_loss: 0.0031\n",
            "Epoch 24/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - loss: 0.0031 - val_loss: 0.0030\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 557ms/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "win_size exceeds image extent. Either ensure that your images are at least 7x7; or pass win_size explicitly in the function call, with an odd value less than or equal to the smaller side of your images. If your images are multichannel (with color channels), set channel_axis to the axis number corresponding to the channels.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-16a90e9b3f8c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# Evaluate quality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mpsnr_traditional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mssim_traditional\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_image_quality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraditional\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mpsnr_autoencoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mssim_autoencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_image_quality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautoencoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-50-36fd3e5c679f>\u001b[0m in \u001b[0;36mevaluate_image_quality\u001b[0;34m(original, enhanced)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mwin_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwin_size\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mwin_size\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mwin_size\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m  \u001b[0;31m# Ensure win_size is odd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mssim_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mssim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menhanced\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwin_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwin_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpsnr_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mssim_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/skimage/metrics/_structural_similarity.py\u001b[0m in \u001b[0;36mstructural_similarity\u001b[0;34m(im1, im2, win_size, gradient, data_range, channel_axis, gaussian_weights, full, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mwin_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    187\u001b[0m             \u001b[0;34m'win_size exceeds image extent. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0;34m'Either ensure that your images are '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: win_size exceeds image extent. Either ensure that your images are at least 7x7; or pass win_size explicitly in the function call, with an odd value less than or equal to the smaller side of your images. If your images are multichannel (with color channels), set channel_axis to the axis number corresponding to the channels."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_traditional_enhancement(image):\n",
        "    \"\"\"\n",
        "    Apply traditional image enhancement techniques.\n",
        "    Args:\n",
        "        image (np.array): Input image.\n",
        "    Returns:\n",
        "        np.array: Enhanced image.\n",
        "    \"\"\"\n",
        "    # Histogram Equalization\n",
        "    image = cv2.equalizeHist((image * 255).astype(np.uint8))\n",
        "    # Gaussian Blurring\n",
        "    image = cv2.GaussianBlur(image, (5, 5), 0)\n",
        "\n",
        "    # Reshape to add channel dimension\n",
        "    image = image[..., np.newaxis] / 255.0  # Add channel dimension and normalize\n",
        "\n",
        "    return image # Ensure the enhanced image also has a channel dimension"
      ],
      "metadata": {
        "id": "mF82_M7NUc25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply traditional enhancement\n",
        "enhanced_traditional = np.array([apply_traditional_enhancement(img) for img in images])\n",
        "\n",
        "# Train Autoencoder\n",
        "autoencoder = build_autoencoder()\n",
        "autoencoder.fit(images, images, epochs=50, batch_size=32, validation_split=0.2, callbacks=[EarlyStopping(patience=3)])\n",
        "enhanced_autoencoder = autoencoder.predict(images)\n",
        "\n",
        "# Train GAN (Simplified)\n",
        "generator, discriminator = build_gan()\n",
        "# Note: GAN training is simplified for demonstration purposes.\n",
        "\n",
        "# Evaluate and visualize results\n",
        "for i in range(min(5, len(images))):  # Display first 5 images\n",
        "    original = images[i]\n",
        "    traditional = enhanced_traditional[i]\n",
        "    autoencoded = enhanced_autoencoder[i]\n",
        "\n",
        "    # Evaluate quality\n",
        "    psnr_traditional, ssim_traditional = evaluate_image_quality(original, traditional)\n",
        "    psnr_autoencoded, ssim_autoencoded = evaluate_image_quality(original, autoencoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Uxw5iSZ5by--",
        "outputId": "c4649289-3b66-4cfd-ecff-252f485c1d8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 2s/step - loss: 0.0703 - val_loss: 0.0165\n",
            "Epoch 2/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 2s/step - loss: 0.0132 - val_loss: 0.0077\n",
            "Epoch 3/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 2s/step - loss: 0.0079 - val_loss: 0.0073\n",
            "Epoch 4/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 2s/step - loss: 0.0073 - val_loss: 0.0064\n",
            "Epoch 5/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - loss: 0.0065 - val_loss: 0.0062\n",
            "Epoch 6/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 2s/step - loss: 0.0059 - val_loss: 0.0053\n",
            "Epoch 7/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - loss: 0.0056 - val_loss: 0.0047\n",
            "Epoch 8/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 2s/step - loss: 0.0048 - val_loss: 0.0046\n",
            "Epoch 9/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - loss: 0.0047 - val_loss: 0.0045\n",
            "Epoch 10/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - loss: 0.0044 - val_loss: 0.0041\n",
            "Epoch 11/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - loss: 0.0042 - val_loss: 0.0039\n",
            "Epoch 12/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - loss: 0.0041 - val_loss: 0.0041\n",
            "Epoch 13/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 2s/step - loss: 0.0040 - val_loss: 0.0036\n",
            "Epoch 14/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - loss: 0.0038 - val_loss: 0.0034\n",
            "Epoch 15/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - loss: 0.0037 - val_loss: 0.0033\n",
            "Epoch 16/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 2s/step - loss: 0.0036 - val_loss: 0.0033\n",
            "Epoch 17/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 2s/step - loss: 0.0035 - val_loss: 0.0034\n",
            "Epoch 18/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - loss: 0.0034 - val_loss: 0.0032\n",
            "Epoch 19/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 2s/step - loss: 0.0033 - val_loss: 0.0031\n",
            "Epoch 20/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 2s/step - loss: 0.0032 - val_loss: 0.0030\n",
            "Epoch 21/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - loss: 0.0032 - val_loss: 0.0029\n",
            "Epoch 22/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - loss: 0.0031 - val_loss: 0.0028\n",
            "Epoch 23/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 2s/step - loss: 0.0030 - val_loss: 0.0028\n",
            "Epoch 24/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 2s/step - loss: 0.0030 - val_loss: 0.0027\n",
            "Epoch 25/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - loss: 0.0029 - val_loss: 0.0026\n",
            "Epoch 26/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - loss: 0.0028 - val_loss: 0.0026\n",
            "Epoch 27/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - loss: 0.0028 - val_loss: 0.0025\n",
            "Epoch 28/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - loss: 0.0028 - val_loss: 0.0025\n",
            "Epoch 29/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 2s/step - loss: 0.0027 - val_loss: 0.0025\n",
            "Epoch 30/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - loss: 0.0027 - val_loss: 0.0024\n",
            "Epoch 31/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - loss: 0.0027 - val_loss: 0.0024\n",
            "Epoch 32/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - loss: 0.0026 - val_loss: 0.0024\n",
            "Epoch 33/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - loss: 0.0026 - val_loss: 0.0023\n",
            "Epoch 34/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - loss: 0.0026 - val_loss: 0.0023\n",
            "Epoch 35/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 2s/step - loss: 0.0025 - val_loss: 0.0023\n",
            "Epoch 36/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 2s/step - loss: 0.0025 - val_loss: 0.0023\n",
            "Epoch 37/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 2s/step - loss: 0.0027 - val_loss: 0.0024\n",
            "Epoch 38/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 2s/step - loss: 0.0026 - val_loss: 0.0022\n",
            "Epoch 39/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - loss: 0.0025 - val_loss: 0.0022\n",
            "Epoch 40/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - loss: 0.0024 - val_loss: 0.0022\n",
            "Epoch 41/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - loss: 0.0024 - val_loss: 0.0022\n",
            "Epoch 42/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - loss: 0.0023 - val_loss: 0.0023\n",
            "Epoch 43/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 2s/step - loss: 0.0024 - val_loss: 0.0022\n",
            "Epoch 44/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 2s/step - loss: 0.0023 - val_loss: 0.0022\n",
            "Epoch 45/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 2s/step - loss: 0.0023 - val_loss: 0.0021\n",
            "Epoch 46/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - loss: 0.0023 - val_loss: 0.0021\n",
            "Epoch 47/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - loss: 0.0023 - val_loss: 0.0021\n",
            "Epoch 48/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 2s/step - loss: 0.0023 - val_loss: 0.0021\n",
            "Epoch 49/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - loss: 0.0022 - val_loss: 0.0020\n",
            "Epoch 50/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 2s/step - loss: 0.0022 - val_loss: 0.0020\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 531ms/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "win_size exceeds image extent. Either ensure that your images are at least 7x7; or pass win_size explicitly in the function call, with an odd value less than or equal to the smaller side of your images. If your images are multichannel (with color channels), set channel_axis to the axis number corresponding to the channels.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-90854e2af6c0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Evaluate quality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mpsnr_traditional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mssim_traditional\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_image_quality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraditional\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mpsnr_autoencoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mssim_autoencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_image_quality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautoencoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-50-36fd3e5c679f>\u001b[0m in \u001b[0;36mevaluate_image_quality\u001b[0;34m(original, enhanced)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mwin_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwin_size\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mwin_size\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mwin_size\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m  \u001b[0;31m# Ensure win_size is odd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mssim_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mssim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menhanced\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwin_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwin_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpsnr_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mssim_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/skimage/metrics/_structural_similarity.py\u001b[0m in \u001b[0;36mstructural_similarity\u001b[0;34m(im1, im2, win_size, gradient, data_range, channel_axis, gaussian_weights, full, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mwin_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    187\u001b[0m             \u001b[0;34m'win_size exceeds image extent. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0;34m'Either ensure that your images are '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: win_size exceeds image extent. Either ensure that your images are at least 7x7; or pass win_size explicitly in the function call, with an odd value less than or equal to the smaller side of your images. If your images are multichannel (with color channels), set channel_axis to the axis number corresponding to the channels."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_image_quality(original, enhanced):\n",
        "    \"\"\"\n",
        "    Evaluate image quality using PSNR and SSIM.\n",
        "    Args:\n",
        "        original (np.array): Original image.\n",
        "        enhanced (np.array): Enhanced image.\n",
        "    Returns:\n",
        "        tuple: PSNR and SSIM scores.\n",
        "    \"\"\"\n",
        "    psnr_score = psnr(original, enhanced, data_range=1)\n",
        "\n",
        "    # Adjust win_size based on image dimensions\n",
        "    win_size = min(7, min(original.shape[0], original.shape[1]))  # Set to 7 or smaller\n",
        "    if win_size % 2 == 0:  # Ensure win_size is odd\n",
        "        win_size -= 1\n",
        "\n",
        "    ssim_score = ssim(original, enhanced, data_range=1, win_size=win_size)\n",
        "    return psnr_score, ssim_score"
      ],
      "metadata": {
        "id": "EGLzkBUNzIOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_results(original, traditional, autoencoded):\n",
        "    \"\"\"Visualizes the results of image enhancement techniques.\n",
        "    Args:\n",
        "        original (np.array): Original image.\n",
        "        traditional (np.array): Traditionally enhanced image.\n",
        "        autoencoded (np.array): Autoencoder enhanced image.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(original.squeeze(), cmap='gray')\n",
        "    plt.title(\"Original Image\")\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(traditional.squeeze(), cmap='gray')\n",
        "    plt.title(f\"Traditional\\nPSNR: {psnr_traditional:.2f}, SSIM: {ssim_traditional:.2f}\")\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.imshow(autoencoded.squeeze(), cmap='gray')\n",
        "    plt.title(f\"Autoencoder\\nPSNR: {psnr_autoencoded:.2f}, SSIM: {ssim_autoencoded:.2f}\")\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "i9nU0fSeWB30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dsQUNgT4ZpfT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}